{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be6433d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "using BackwordsDiffLibrary\n",
    "using Random\n",
    "using Statistics \n",
    "\n",
    "abstract type Layer end\n",
    "\n",
    "struct Dense <: Layer\n",
    "    W::Array{Float32,2}\n",
    "    b::Array{Float32,1}\n",
    "    activation::Function     \n",
    "end\n",
    "\n",
    "function Dense(in_features::Int, out_features::Int, activation::Function)\n",
    "    weights = glorot_uniform(out_features, in_features)\n",
    "    baias = zeros(out_features)\n",
    "    return Dense(weights, baias, activation)\n",
    "end\n",
    "\n",
    "function glorot_uniform(fan_out::Int, fan_in::Int)\n",
    "    limit = sqrt(6f0 / (fan_in + fan_out))\n",
    "    # rand(fan_out, fan_in) generates values in [0, 1), so we scale them to [-limit, limit)\n",
    "    return rand(Float32, fan_out, fan_in) .* (2f0 * limit) .- limit\n",
    "end\n",
    "\n",
    "function (layer::Dense)(x::AbstractArray)\n",
    "    z = layer.W * x .+ layer.b  \n",
    "    return layer.activation.(z)\n",
    "end\n",
    "\n",
    "struct MLP\n",
    "    layers::Vector{Layer}\n",
    "end\n",
    "\n",
    "function MLP(layers::Vector{Layer})\n",
    "    return MLP(layers)    \n",
    "end\n",
    "\n",
    "# --- Optimizers ---\n",
    "\n",
    "abstract type Optimiser end\n",
    "\n",
    "struct SGD <: Optimiser\n",
    "    lr::Float64\n",
    "end\n",
    "\n",
    "function update!(opt::SGD, ps, grads)\n",
    "    for i in eachindex(ps)\n",
    "        ps[i] .-= opt.lr .* grads[i]\n",
    "    end\n",
    "end\n",
    "\n",
    "struct Adam <: Optimiser\n",
    "    lr::Float32\n",
    "    beta1::Float32\n",
    "    beta2::Float32\n",
    "    eps::Float32\n",
    "    m::Vector{Array{Float32}}\n",
    "    v::Vector{Array{Float32}}\n",
    "    t::Vector{Int}\n",
    "end\n",
    "\n",
    "function Adam(lr::Float32, ps; beta1=0.9f0, beta2=0.999f0, eps=1e-8)\n",
    "    m = [zeros(Float32, size(p)) for p in ps]\n",
    "    v = [zeros(size(p)) for p in ps]\n",
    "    Adam(lr, beta1, beta2, convert(Float32, eps), m, v, [0])\n",
    "end\n",
    "\n",
    "function update!(opt::Adam, ps, grads)\n",
    "    opt.t[1] += 1\n",
    "    for i in eachindex(ps)\n",
    "        opt.m[i] .= opt.beta1 .* opt.m[i] .+ (1 - opt.beta1) .* grads[i]\n",
    "        opt.v[i] .= opt.beta2 .* opt.v[i] .+ (1 - opt.beta2) .* (grads[i].^2)\n",
    "        m_hat = opt.m[i] ./ (1 - opt.beta1^opt.t[1])\n",
    "        v_hat = opt.v[i] ./ (1 - opt.beta2^opt.t[1])\n",
    "        ps[i] .-= opt.lr .* m_hat ./ (sqrt.(v_hat) .+ opt.eps)\n",
    "    end\n",
    "end\n",
    "\n",
    "# --- Passes ---\n",
    "\n",
    "function forward(model::MLP, x::Array)\n",
    "    A = Vector{AbstractMatrix{Float32}}()\n",
    "    Z = Vector{AbstractMatrix{Float32}}()\n",
    "\n",
    "    push!(A, x)\n",
    "\n",
    "    for layer in model.layers\n",
    "        z = layer.W * x .+ layer.b\n",
    "        x = layer.activation.(z)\n",
    "\n",
    "        push!(A, x)\n",
    "        push!(Z, z)\n",
    "    end\n",
    "    return x, A, Z\n",
    "end\n",
    "\n",
    "function backword(loss_grad, model::MLP, A, Z, epoch, n)\n",
    "    dA_prev = loss_grad\n",
    "    gradients = []\n",
    "\n",
    "    for i in length(model.layers):-1:1\n",
    "        layer = model.layers[i]\n",
    "        W = layer.W\n",
    "        b = layer.b\n",
    "        @diffunction d_activation(x) = layer.activation.(x) \n",
    "\n",
    "        z = Z[i]\n",
    "        a = A[i]\n",
    "\n",
    "        dZ = dA_prev .* grad(d_activation, [z])[1]\n",
    "\n",
    "        dW = (dZ * a')\n",
    "        db = sum(dZ, dims=2)\n",
    "        dA_prev = W' * dZ\n",
    "\n",
    "        B = size(a, 2)\n",
    "        dW ./= B\n",
    "        db ./= B\n",
    "\n",
    "        push!(gradients, db)\n",
    "        push!(gradients, dW)\n",
    "\n",
    "        dA_prev = W' * dZ\n",
    "    end\n",
    "\n",
    "    reverse!(gradients)\n",
    "    return gradients\n",
    "end\n",
    "\n",
    "# --- Helpers ---\n",
    "\n",
    "function batch_accuracy(y_pred::Array, y_true::Array)\n",
    "    if size(y_pred,1)==1\n",
    "      # binary: predict “1” if p≥0.5, else “0”\n",
    "      preds = vec(y_pred .>= 0.5f0)\n",
    "      trues = vec(y_true .== 1f0)\n",
    "    else\n",
    "      # multi‑class: usual argmax\n",
    "      preds = vec(argmax(y_pred, dims=1))\n",
    "      trues = vec(argmax(y_true, dims=1))\n",
    "    end\n",
    "    return mean(preds .== trues)\n",
    "end\n",
    "\n",
    "function get_params(model::MLP)\n",
    "    ps = []\n",
    "    for layer in model.layers\n",
    "        push!(ps, layer.W)\n",
    "        push!(ps, layer.b)\n",
    "    end\n",
    "    return ps\n",
    "end\n",
    "\n",
    "# --- Training ---\n",
    "\n",
    "function train!(model::MLP, loss_fun, X_train, y_train, X_test, y_test;\n",
    "    epochs=5, lr=0.001, batchsize=64, optimizer=:SGD)\n",
    "\n",
    "    # Extract parameters for possible use by the optimizer.\n",
    "    ps = get_params(model)\n",
    "    opt = nothing\n",
    "    if optimizer == :SGD\n",
    "        opt = SGD(lr)\n",
    "    elseif optimizer == :Adam\n",
    "        opt = Adam(convert(Float32, lr), ps)\n",
    "    else\n",
    "        error(\"Unsupported optimizer type. Choose :SGD or :Adam.\")\n",
    "    end\n",
    "\n",
    "    n_samples = size(X_train, 2)  # assuming samples are arranged in columns\n",
    "\n",
    "    for epoch in 1:epochs\n",
    "        total_loss = 0.0\n",
    "        total_acc = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        # Process mini-batches.\n",
    "        for i in 1:batchsize:n_samples\n",
    "            last = min(i + batchsize - 1, n_samples)\n",
    "            x_batch = X_train[:, i:last]\n",
    "            y_batch = y_train[:, i:last]\n",
    "\n",
    "            # FORWARD PASS\n",
    "            y_pred, A, Z = forward(model, x_batch)\n",
    "            \n",
    "            # Compute current loss value for reporting.\n",
    "            batch_loss = loss_fun(y_pred, y_batch)\n",
    "\n",
    "            # accuracy\n",
    "            batch_acc = batch_accuracy(y_pred, y_batch)\n",
    "\n",
    "            # Compute Loss Gradient \n",
    "            @diffunction loss_wrapper(a, y) = loss_fun(a, y)\n",
    "            delta = grad(loss_wrapper, [y_pred, y_batch])[1]\n",
    "            \n",
    "            # BACKWARD PASS\n",
    "            grads = backword(delta, model, A, Z, epoch, num_batches)\n",
    "\n",
    "            update!(opt, ps, grads)\n",
    "\n",
    "            # accumulate\n",
    "            total_loss += batch_loss\n",
    "            total_acc  += batch_acc\n",
    "            num_batches += 1\n",
    "        end\n",
    "\n",
    "        # average over all batches\n",
    "        avg_train_loss = total_loss / num_batches\n",
    "        avg_train_acc  = total_acc  / num_batches\n",
    "\n",
    "        # optional validation on test set\n",
    "        y_test_pred, _, _ = forward(model, Matrix(X_test))\n",
    "        test_loss = loss_fun(y_test_pred, y_test)\n",
    "        test_acc  = batch_accuracy(y_test_pred, y_test)\n",
    "\n",
    "        println(\n",
    "            \"Epoch $epoch ▶ \",\n",
    "            \"Train Loss=$(round(avg_train_loss, digits=4)), \",\n",
    "            \"Train Acc=$(round(100*avg_train_acc, digits=2))%  │ \",\n",
    "            \"Test Loss=$(round(test_loss, digits=4)), \",\n",
    "            \"Test Acc=$(round(100*test_acc, digits=2))%\"\n",
    "        )\n",
    "    end\n",
    "end\n",
    "\n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33a8a9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "using JLD2\n",
    "X_train = load(\"data/imdb_dataset_prepared.jld2\", \"X_train\")\n",
    "y_train = load(\"data/imdb_dataset_prepared.jld2\", \"y_train\")\n",
    "y_train = Float32.(y_train)\n",
    "X_test = load(\"data/imdb_dataset_prepared.jld2\", \"X_test\")\n",
    "y_test = load(\"data/imdb_dataset_prepared.jld2\", \"y_test\")\n",
    "y_test  = Float32.(y_test)\n",
    "\n",
    "# --- Model Definition ---\n",
    "\n",
    "loss_fun(y_pred, y_true) = -mean(y_true .* log.(y_pred .+ 1e-7) .+ (1 .- y_true) .* log.(1 .- y_pred .+ 1e-7))\n",
    "\n",
    "in_features = size(X_train, 1)\n",
    "hidden = 32\n",
    "out_features = 1\n",
    "\n",
    "layers = [Dense(in_features, hidden, ReLU), Dense(hidden, out_features, Sigmoid)]\n",
    "model = MLP(layers)\n",
    "\n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6df72b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ▶ Train Loss=0.6463, Train Acc=77.81%  │ Test Loss=0.5795, Test Acc=82.65%\n",
      "Epoch 2 ▶ Train Loss=0.46, Train Acc=90.72%  │ Test Loss=0.4401, Test Acc=85.85%\n",
      "Epoch 3 ▶ Train Loss=0.2991, Train Acc=94.11%  │ Test Loss=0.3658, Test Acc=87.3%\n",
      "Epoch 4 ▶ Train Loss=0.2045, Train Acc=96.26%  │ Test Loss=0.3323, Test Acc=87.0%\n",
      "Epoch 5 ▶ Train Loss=0.1464, Train Acc=97.65%  │ Test Loss=0.3185, Test Acc=87.15%\n"
     ]
    }
   ],
   "source": [
    "train!(model, loss_fun, X_train, y_train, X_test, y_test; epochs=5, lr=0.001, batchsize=64, optimizer=:Adam)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
